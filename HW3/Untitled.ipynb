{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from enum import Enum\n",
    "import re\n",
    "\n",
    "import lxml.etree as et\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "\n",
    "class Tag(Enum):\n",
    "    NONE = 0,\n",
    "    ORG = 1,\n",
    "    PERSON = 2\n",
    "\n",
    "\n",
    "tag_map = {\n",
    "    \"Name\": Tag.PERSON,\n",
    "    \"Surn\": Tag.PERSON,\n",
    "    \"Patr\": Tag.PERSON,\n",
    "    \"Orgn\": Tag.ORG,\n",
    "    \"Trad\": Tag.ORG,\n",
    "}\n",
    "\n",
    "\n",
    "not_names = set([word.strip() for word in open(os.path.join(\"resources/russian\"), \"r\")])\n",
    "org_pattern = re.compile(r\"^['\\-\\\"\\.&a-zA-z]+$\")\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, tag=Tag.NONE):\n",
    "        self.tag = tag\n",
    "        self.edges = {}\n",
    "\n",
    "    def get_first_match(self, words):\n",
    "        if self.tag != Tag.NONE:\n",
    "            return self.tag, 0\n",
    "        if len(words) == 0:\n",
    "            return Tag.NONE, 0\n",
    "        if words[0] in self.edges:\n",
    "            tag, size = self.edges[words[0]].get_first_match(words[1:])\n",
    "            return tag, size + 1 if tag != Tag.NONE else 0\n",
    "        return Tag.NONE, 0\n",
    "\n",
    "    def add(self, words, tag):\n",
    "        if len(words) == 0:\n",
    "            self.tag = tag\n",
    "            return\n",
    "        if words[0] not in self.edges:\n",
    "            self.edges[words[0]] = Node()\n",
    "        self.edges[words[0]].add(words[1:], tag)\n",
    "\n",
    "    def add_all(self, words, tag):\n",
    "        for word in words:\n",
    "            if word.lower() in not_names:\n",
    "                continue\n",
    "            if word not in self.edges:\n",
    "                self.edges[word] = Node()\n",
    "            self.edges[word].tag = tag\n",
    "\n",
    "\n",
    "mystem = Mystem()\n",
    "tokenizer = RegexpTokenizer(r'((?:[.\\-\"\\'&]?\\w[.\\-\"\\'&]?)+)')\n",
    "root = Node()\n",
    "def lemmatize(words):\n",
    "    return [\"\".join([s.strip() for s in mystem.lemmatize(word)]) for word in words]\n",
    "\n",
    "\n",
    "def parse_dict(org_tag, per_tag, files, words):\n",
    "    for train_file_name in files:\n",
    "        with open(os.path.join(train_file_name), \"r\") as train_file:\n",
    "            for line in train_file:\n",
    "                tag = line.split()[1]\n",
    "                word = words(line)\n",
    "                if tag == org_tag:\n",
    "                    if len(word) == 1 and (word[0] == \"большой\" or word[0] == \"Украины\"):\n",
    "                        print(line)\n",
    "                        continue\n",
    "                    if len(word) == 1:\n",
    "                        root.add(lemmatize([f'\"{word[0]}\"']), Tag.ORG)\n",
    "                    root.add(lemmatize(word), Tag.ORG)\n",
    "                elif tag == per_tag:\n",
    "                    if len(word) == 1 and (word[0] == \"Украины\" or word[0] in not_names):\n",
    "                        print(line)\n",
    "                        continue\n",
    "                    root.add_all(word, Tag.PERSON)\n",
    "\n",
    "\n",
    "def preprocess(s):\n",
    "    return s.replace('«', '\"').replace('»', '\"').replace('ё', 'e')\n",
    "\n",
    "\n",
    "def tokenize1(s):\n",
    "    s = preprocess(s)\n",
    "    return tokenizer.tokenize(s)[4:]\n",
    "\n",
    "\n",
    "def tokenize2(s):\n",
    "    s = preprocess(s)\n",
    "    s = s.split(\"#\")[1]\n",
    "    return tokenizer.tokenize(s)\n",
    "\n",
    "\n",
    "def predict(s):\n",
    "    word = s[0]\n",
    "    if word[0] == '\"' and word[-1] == '\"' and len(word) > 1 and word[1].isupper():\n",
    "        return Tag.ORG, 1\n",
    "    if len(s) > 1:\n",
    "        word2 = s[1]\n",
    "        if word[0] == '\"' and word2[-1] == '\"' and len(word) > 1 and word[1].isupper():\n",
    "            return Tag.ORG, 2\n",
    "    #size = 0\n",
    "    #while size < len(s) and org_pattern.match(s[size]):\n",
    "    #    size += 1\n",
    "    #return Tag.NONE if size == 0 else Tag.ORG, size\n",
    "    return Tag.NONE, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T2\tORG 43 50\tУкраины\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parse_dict(\"ORG\", \"PER\", glob.glob(\"resources/Collection5/*.ann\"), lambda s : tokenize1(s))\n",
    "parse_dict(\"Org\", \"Person\", glob.glob(\"resources/testset/*.objects\"), lambda s : tokenize2(s))\n",
    "context = et.iterparse(\"resources/dict.opcorpora.xml\", tag='lemma')\n",
    "for (_, element) in context:\n",
    "    tag = Tag.NONE\n",
    "    lemma = element[0]\n",
    "    for g in lemma:\n",
    "        if g.attrib['v'] in tag_map:\n",
    "            tag = tag_map[g.attrib['v']]\n",
    "    if tag != Tag.NONE:\n",
    "        #print(lemma.attrib['t'])\n",
    "        for form in element[1:]:\n",
    "            word = form.attrib['t'].capitalize()\n",
    "            if word.lower() in not_names:\n",
    "                continue\n",
    "            if tag == Tag.ORG:\n",
    "                root.add(lemmatize([word]), tag)\n",
    "                root.add(lemmatize([f'\"{word}\"']), tag)\n",
    "            else:\n",
    "                root.add(lemmatize([word]), tag)\n",
    "    element.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result.txt\", \"w\") as result_file:\n",
    "    with open(\"resources/dataset_40163_1.txt\", \"r\") as dataset:\n",
    "        for sentence in dataset:\n",
    "            sentence = preprocess(sentence)\n",
    "            if sentence[-2] == '.':\n",
    "                sentence = sentence[:-2]\n",
    "            tokens = lemmatize(tokenizer.tokenize(sentence))\n",
    "            positions = list(tokenizer.span_tokenize(sentence))\n",
    "            current_index = 0\n",
    "            while current_index < len(tokens):\n",
    "                (tag, size) = root.get_first_match(tokens[current_index:])\n",
    "                #if tag == Tag.NONE:\n",
    "                #    (tag, size) = predict(tokens[current_index:])\n",
    "                for index in range(current_index, current_index + size):\n",
    "                    result_file.write(f\"{positions[index][0]} {positions[index][1] - positions[index][0]} {tag.name} \")\n",
    "                if tag == Tag.NONE:\n",
    "                    size = 1\n",
    "                current_index += size\n",
    "            result_file.write(\"EOL\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
