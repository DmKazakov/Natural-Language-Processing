{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk import ToktokTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pymystem3 import Mystem\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "stop_words = set([word.strip() for word in open(os.path.join(\"resources/russian\"), \"r\")])\n",
    "mystem = Mystem()\n",
    "tokenizer = ToktokTokenizer()\n",
    "tfidf = TfidfVectorizer()\n",
    "indexes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParaQuestion:\n",
    "    def __init__(self, para, question, id, answer): \n",
    "        self.para, unstemmed = parse_text(para)\n",
    "        self.question = set(parse_text(question)[0][0])\n",
    "        self.id = id\n",
    "        closest_sent_ind = self.calc_closest()\n",
    "        self.closest_sent = self.para[closest_sent_ind]\n",
    "        self.closest_sent_unst = unstemmed[closest_sent_ind]\n",
    "        self.answer = None if answer is None else parse_text(answer)[0][0]\n",
    "        self.ans_vec = None\n",
    "\n",
    "    def para_words(self):\n",
    "        return ' '.join([word for sent in self.para for word in sent if self.answer is not None or word in indexes])\n",
    "\n",
    "    def tfidf(self, word):\n",
    "        if word not in indexes:\n",
    "            return 0\n",
    "        return self.para_tfidf[0, indexes[word]]\n",
    "\n",
    "    def precalc(self):\n",
    "        self.para_tfidf = tfidf.transform([self.para_words()])\n",
    "        self.sums = [0]\n",
    "        self.sums_m = [0]\n",
    "        for i in range(len(self.closest_sent)):\n",
    "            self.sums_m.append(self.sums_m[i])\n",
    "            val = self.tfidf(self.closest_sent[i])\n",
    "            if self.closest_sent[i] in self.question:\n",
    "                self.sums_m[i + 1] += val\n",
    "            self.sums.append(self.sums[i] + val)\n",
    "\n",
    "    def to_vectors(self):\n",
    "        self.precalc()\n",
    "        vectors = []\n",
    "        for right in range(len(self.closest_sent)):\n",
    "            for left in range(right + 1):\n",
    "                span_len = right - left + 1\n",
    "                sent_len = len(self.closest_sent)\n",
    "                left_len = left\n",
    "                right_len = sent_len - left_len - span_len\n",
    "                span_tfidf = self.sums[right + 1] - self.sums[left]\n",
    "                span_match_tfidf = self.sums_m[right + 1] - self.sums_m[left]\n",
    "                sent_tfidf = self.sums_m[len(self.closest_sent)]\n",
    "                left_tfidf = self.sums_m[left]\n",
    "                right_tfidf = self.sums_m[len(self.closest_sent)] - self.sums_m[right + 1]\n",
    "                vectors.append([span_len, sent_len, left_len, right_len, span_tfidf, span_match_tfidf, sent_tfidf, left_tfidf, right_tfidf])\n",
    "\n",
    "                if self.answer is not None and self.answer == self.closest_sent[left : right + 1]:\n",
    "                    self.ans_vec = len(vectors) - 1\n",
    "\n",
    "        return vectors\n",
    "\n",
    "    def calc_closest(self):\n",
    "        max_match = 0\n",
    "        index = 0\n",
    "        for ind in range(len(self.para)):\n",
    "            match = 0\n",
    "            marked = set()\n",
    "            for word in self.para[ind]:\n",
    "                if word in self.question and word not in stop_words and word not in marked:\n",
    "                    match += 1\n",
    "                    marked.add(word)\n",
    "            if match >= max_match:\n",
    "                max_match = match\n",
    "                index = ind\n",
    "        return index\n",
    "\n",
    "\n",
    "def parse_text(text):\n",
    "    sentences = nltk.sent_tokenize(text, language=\"russian\")\n",
    "    sentences = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "    return [[process_word(word, True) for word in sentence if word not in string.punctuation] for sentence in sentences], [[word for word in sentence if word not in string.punctuation] for sentence in sentences]\n",
    "\n",
    "\n",
    "def process_word(word, lemma):\n",
    "    word = word.lower()\n",
    "    if word[-1] == \"[\" or word[-1] == '.':\n",
    "        word = word[:-1]\n",
    "    return \"\".join([s.strip() for s in mystem.lemmatize(word)]) if lemma else word\n",
    "\n",
    "\n",
    "def read_csv(file_name, sep=','):\n",
    "    csv = pd.read_csv(file_name, sep=sep)\n",
    "    return [ParaQuestion(para[2], para[3], para[1], para[4] if len(para) > 4 else None) for para in csv.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_csv(\"resources/dataset_281937_1.txt\", '\\t')\n",
    "tfidf.fit([doc.para_words() for doc in read_csv(\"resources/train_qa.csv\")] + [doc.para_words() for doc in test])\n",
    "features = tfidf.get_feature_names()\n",
    "for i in range(len(features)):\n",
    "    indexes[features[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5922\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "labels = []\n",
    "cnt = 0\n",
    "for doc in train:\n",
    "    vecs = doc.to_vectors()\n",
    "    vectors.extend(vecs)\n",
    "    if doc.ans_vec is not None:\n",
    "        labels.extend([0] * doc.ans_vec)\n",
    "        labels.append(1)\n",
    "        labels.extend([0] * (len(vecs) - doc.ans_vec - 1))\n",
    "    else:\n",
    "        labels.extend([0] * len(vecs))\n",
    "        cnt += 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vectors.csv', mode='w') as vectors_file:\n",
    "    writer = csv.writer(vectors_file, delimiter='\\t')\n",
    "    for i in range(len(vectors)):\n",
    "        writer.writerow(vectors[i] + [labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230411\n"
     ]
    }
   ],
   "source": [
    "#csv = pd.read_csv(\"vectors.csv\", sep=\"\\t\")\n",
    "vectors = []\n",
    "labels = []\n",
    "for vector in csv.values:\n",
    "    if vector[-1] == 1 or random.random() < 0.008:\n",
    "        vectors.append(vector[:-1])\n",
    "        labels.append(vector[-1])\n",
    "print(len(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "       importance_type='gain', learning_rate=0.33, max_delta_step=0,\n",
       "       max_depth=5, min_child_weight=1, missing=None, n_estimators=300,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf = XGBRegressor(objective='binary:logistic', n_estimators=300, max_depth=5, learning_rate=0.33)\n",
    "xgb_clf.fit(np.array(vectors), np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_csv(\"resources/dataset_281937_1.txt\", '\\t')\n",
    "with open(\"result.txt\", \"w\") as result_file:\n",
    "    for doc in test:\n",
    "       vecs = doc.to_vectors()\n",
    "       pr = xgb_clf.predict(vecs)\n",
    "       ind = np.argmax(pr)\n",
    "       left = vecs[ind][2]\n",
    "       right = vecs[ind][0] + left - 1\n",
    "       result_file.write(f\"{doc.id}\\t{' '.join(doc.closest_sent_unst[left:right + 1])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
